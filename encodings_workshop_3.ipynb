{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426a7f28-a94b-4f38-b03a-6225443a919f",
   "metadata": {},
   "source": [
    "## 6. User Interaction: Interactive Encoding Workshop\n",
    "\n",
    "This section provides a comprehensive interactive environment to experiment with different text encoding techniques and parameters. You can:\n",
    "\n",
    "- Try various encoding methods (Bag of Words, TF-IDF, One-Hot Encoding, Word Embeddings)\n",
    "- Adjust preprocessing parameters\n",
    "- Compare results across different encoding approaches\n",
    "- Explore how changing parameters affects the resulting encodings\n",
    "- Visualize the impact of your choices on dimensionality and information preservation\n",
    "\n",
    "This hands-on approach helps build intuition about which encoding methods work best for different NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae410155-73c2-480e-a756-d64f49089c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad269595e6544262affb6d01f9975421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Natural language processing is a subfield of artificial intelligence.\\nMachine learning algori…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3403c4aed5148df8accf805fcdd29e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Text Preprocessing Options:</b>'), HBox(children=(Checkbox(value=True, descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5fb98714e34e669bf844f052ca82a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Method:', options=(('Bag of Words (Count)', 'count'), ('TF-IDF', 'tfidf')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05747c1c74ea4295a18de7fe6513c992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Advanced Encoding Options:</b>'), HBox(children=(IntSlider(value=100, continuous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7940e1bc5f4c0d96762c963c0bcdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Run Encoding', style=ButtonStyle(), tooltip='Click to run the enco…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca2ba8162d542159d2162bd7272c171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Sample corpus for demonstrations\n",
    "sample_corpus = [\n",
    "    \"Natural language processing is a subfield of artificial intelligence.\",\n",
    "    \"Machine learning algorithms can perform text classification tasks.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\",\n",
    "    \"Text preprocessing is crucial for effective language processing.\",\n",
    "    \"Deep learning models have revolutionized natural language understanding.\"\n",
    "]\n",
    "\n",
    "def preprocess_text(text, lowercase=True, remove_punctuation=True, \n",
    "                   remove_stopwords=False, lemmatize=False):\n",
    "    \"\"\"Preprocess text with configurable options\"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def encode_text(texts, method=\"count\", max_features=None, binary=False,\n",
    "               ngram_range=(1,1), min_df=1, max_df=1.0, use_idf=True):\n",
    "    \"\"\"Encode text using different methods\"\"\"\n",
    "    preprocessed_texts = [' '.join(preprocess_text(text, \n",
    "                                                  lowercase=preprocess_options['lowercase'],\n",
    "                                                  remove_punctuation=preprocess_options['remove_punct'],\n",
    "                                                  remove_stopwords=preprocess_options['remove_stopwords'],\n",
    "                                                  lemmatize=preprocess_options['lemmatize'])) \n",
    "                          for text in texts]\n",
    "    \n",
    "    if method == \"count\":\n",
    "        vectorizer = CountVectorizer(max_features=max_features,\n",
    "                                    binary=binary,\n",
    "                                    ngram_range=ngram_range,\n",
    "                                    min_df=min_df,\n",
    "                                    max_df=max_df)\n",
    "        X = vectorizer.fit_transform(preprocessed_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features,\n",
    "                                    binary=binary,\n",
    "                                    ngram_range=ngram_range,\n",
    "                                    min_df=min_df,\n",
    "                                    max_df=max_df,\n",
    "                                    use_idf=use_idf)\n",
    "        X = vectorizer.fit_transform(preprocessed_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "    elif method == \"hashing\":\n",
    "        vectorizer = HashingVectorizer(n_features=max_features if max_features else 1024,\n",
    "                                     binary=binary,\n",
    "                                     ngram_range=ngram_range)\n",
    "        X = vectorizer.fit_transform(preprocessed_texts)\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "    elif method == \"onehot\":\n",
    "        # Simple one-hot encoding using CountVectorizer with binary=True\n",
    "        vectorizer = CountVectorizer(max_features=max_features,\n",
    "                                    binary=True,  # One-hot encoding\n",
    "                                    ngram_range=ngram_range,\n",
    "                                    min_df=min_df,\n",
    "                                    max_df=max_df)\n",
    "        X = vectorizer.fit_transform(preprocessed_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "    return X, feature_names, vectorizer\n",
    "\n",
    "def visualize_encoding_results(X, feature_names, texts, method):\n",
    "    \"\"\"Visualize the encoding results\"\"\"\n",
    "    # Create tabs for different visualizations\n",
    "    tab = widgets.Tab()\n",
    "    children = []\n",
    "    \n",
    "    # Tab 1: Matrix Visualization\n",
    "    def create_matrix_viz():\n",
    "        out = widgets.Output()\n",
    "        with out:\n",
    "            # Properly handle feature selection for display\n",
    "            display_features = feature_names[:20] if len(feature_names) > 20 else feature_names\n",
    "            \n",
    "            if isinstance(X, np.ndarray):\n",
    "                # For dense matrices, select columns corresponding to display_features\n",
    "                if len(feature_names) > 20:\n",
    "                    display_data = X[:, :20]\n",
    "                else:\n",
    "                    display_data = X\n",
    "                df = pd.DataFrame(display_data, columns=display_features)\n",
    "            else:\n",
    "                # For sparse matrices\n",
    "                if len(feature_names) > 20:\n",
    "                    # Only take first 20 columns\n",
    "                    display_data = X.tocsc()[:, :20]\n",
    "                else:\n",
    "                    display_data = X\n",
    "                df = pd.DataFrame(display_data.toarray(), columns=display_features)\n",
    "            \n",
    "            # Set index for better display\n",
    "            df.index = [f\"Doc {i+1}\" for i in range(len(texts))]\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "            plt.title(f\"{method.upper()} Encoding Matrix\" + \n",
    "                      (\" (showing first 20 features)\" if len(feature_names) > 20 else \"\"))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display sparse statistics for sparse methods\n",
    "            if hasattr(X, 'nnz'):  # Check if it's a sparse matrix\n",
    "                non_zeros = X.nnz\n",
    "                total_elements = X.shape[0] * X.shape[1]\n",
    "                sparsity = 100 * (1 - non_zeros / total_elements)\n",
    "                display(HTML(f\"<div style='margin-top:10px'><b>Sparsity:</b> {sparsity:.2f}% of elements are zero</div>\"))\n",
    "                display(HTML(f\"<div><b>Non-zero elements:</b> {non_zeros} out of {total_elements}</div>\"))\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    # Tab 2: Document Similarity\n",
    "    def create_similarity_viz():\n",
    "        out = widgets.Output()\n",
    "        with out:\n",
    "            # Compute similarity matrix\n",
    "            if isinstance(X, np.ndarray):\n",
    "                sim_matrix = cosine_similarity(X)\n",
    "            else:\n",
    "                sim_matrix = cosine_similarity(X)  # sklearn handles sparse matrices\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(sim_matrix, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", \n",
    "                        xticklabels=[f\"Doc {i+1}\" for i in range(len(texts))], \n",
    "                        yticklabels=[f\"Doc {i+1}\" for i in range(len(texts))])\n",
    "            plt.title(f\"Document Similarity Matrix ({method.upper()} Encoding)\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Most similar document pairs\n",
    "            if len(texts) > 1:  # Only if we have multiple documents\n",
    "                sim_matrix_copy = sim_matrix.copy()\n",
    "                np.fill_diagonal(sim_matrix_copy, -1)  # Remove self-similarity\n",
    "                most_similar_idx = np.unravel_index(np.argmax(sim_matrix_copy), sim_matrix_copy.shape)\n",
    "                display(HTML(f\"<div style='margin-top:10px'><b>Most similar documents:</b> Doc {most_similar_idx[0]+1} and Doc {most_similar_idx[1]+1} (similarity: {sim_matrix_copy[most_similar_idx]:.2f})</div>\"))\n",
    "                display(HTML(f\"<div style='margin-top:5px'><b>Doc {most_similar_idx[0]+1}:</b> {texts[most_similar_idx[0]]}</div>\"))\n",
    "                display(HTML(f\"<div style='margin-top:5px'><b>Doc {most_similar_idx[1]+1}:</b> {texts[most_similar_idx[1]]}</div>\"))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Tab 3: Dimension Reduction (TSNE) - UPDATED VERSION\n",
    "    def create_tsne_viz():\n",
    "        out = widgets.Output()\n",
    "        with out:\n",
    "            # Perform t-SNE dimension reduction\n",
    "            if X.shape[0] > 2:  # Need at least 3 documents\n",
    "                try:\n",
    "                    # Calculate appropriate perplexity (should be smaller than n_samples)\n",
    "                    # A good rule of thumb is perplexity = min(30, n_samples/3)\n",
    "                    n_samples = X.shape[0]\n",
    "                    perplexity = min(30, max(5, n_samples/5))\n",
    "                    \n",
    "                    if isinstance(X, np.ndarray):\n",
    "                        X_embedded = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(X)\n",
    "                    else:\n",
    "                        X_embedded = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(X.toarray())\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 8))\n",
    "                    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], s=100)\n",
    "                    \n",
    "                    # Add document labels\n",
    "                    for i, (x, y) in enumerate(X_embedded):\n",
    "                        plt.annotate(f\"Doc {i+1}\", (x, y), fontsize=12, \n",
    "                                    alpha=0.8, xytext=(5, 5), textcoords='offset points')\n",
    "                    \n",
    "                    plt.title(f\"t-SNE Visualization of Document Vectors ({method.upper()} Encoding)\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Explain the visualization\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>Interpretation:</b> Documents that are semantically similar are positioned closer together in this 2D representation.</div>\"))\n",
    "                    display(HTML(f\"<div><b>Note:</b> Using perplexity={perplexity:.1f} for {n_samples} documents.</div>\"))\n",
    "                except Exception as e:\n",
    "                    display(HTML(f\"<div style='color:red'>Could not create t-SNE visualization: {str(e)}</div>\"))\n",
    "                    display(HTML(\"<div><b>Explanation:</b> t-SNE requires at least 3 documents with a perplexity value less than the number of documents.</div>\"))\n",
    "            elif X.shape[0] == 2:\n",
    "                # With just 2 documents, we can simply show their similarity without t-SNE\n",
    "                display(HTML(\"<div>t-SNE not necessary for only 2 documents. Displaying simple comparison:</div>\"))\n",
    "                \n",
    "                # Show similarity between the two documents\n",
    "                if isinstance(X, np.ndarray):\n",
    "                    sim = cosine_similarity(X)[0, 1]\n",
    "                else:\n",
    "                    sim = cosine_similarity(X)[0, 1]\n",
    "                    \n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.bar(['Document Similarity'], [sim])\n",
    "                plt.ylim(0, 1)\n",
    "                plt.title(\"Cosine Similarity Between Documents\")\n",
    "                plt.ylabel(\"Similarity Score\")\n",
    "                plt.show()\n",
    "            else:\n",
    "                display(HTML(\"<div>t-SNE visualization requires at least 2 documents.</div>\"))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Tab 4: Feature Importance (for sparse encodings)\n",
    "    def create_feature_importance():\n",
    "        out = widgets.Output()\n",
    "        with out:\n",
    "            if method in [\"count\", \"tfidf\", \"onehot\"]:\n",
    "                if isinstance(X, np.ndarray):\n",
    "                    feature_sums = X.sum(axis=0)\n",
    "                    if isinstance(feature_sums, np.matrix):\n",
    "                        feature_sums = feature_sums.A1  # Convert matrix to array\n",
    "                else:\n",
    "                    feature_sums = X.sum(axis=0).A1  # Convert to 1D array\n",
    "                \n",
    "                # Create dataframe with feature importance\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Importance': feature_sums\n",
    "                })\n",
    "                \n",
    "                # Get top 15 features by importance\n",
    "                top_features = importance_df.sort_values('Importance', ascending=False).head(15)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                bars = sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "                \n",
    "                # Add value labels to bars\n",
    "                for i, v in enumerate(top_features['Importance']):\n",
    "                    bars.text(v + 0.1, i, f\"{v:.2f}\", va='center')\n",
    "                    \n",
    "                plt.title(f\"Top 15 Most Important Features ({method.upper()} Encoding)\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Explain what importance means for each method\n",
    "                if method == 'count':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>Importance in BoW:</b> Total count of each term across all documents. Higher values indicate more frequent terms.</div>\"))\n",
    "                elif method == 'tfidf':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>Importance in TF-IDF:</b> Sum of TF-IDF scores for each term. Higher values indicate terms that are both frequent in some documents and discriminative across the corpus.</div>\"))\n",
    "                elif method == 'onehot':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>Importance in One-Hot:</b> Number of documents containing each term. Higher values indicate more widespread terms.</div>\"))\n",
    "            elif method == 'hashing':\n",
    "                display(HTML(\"<div>Feature importance visualization not applicable for hashing vectorizer (feature names are not interpretable).</div>\"))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # Add tabs\n",
    "    children.append(create_matrix_viz())\n",
    "    children.append(create_similarity_viz())\n",
    "    children.append(create_tsne_viz())\n",
    "    children.append(create_feature_importance())\n",
    "    \n",
    "    tab.children = children\n",
    "    tab.set_title(0, \"Encoding Matrix\")\n",
    "    tab.set_title(1, \"Document Similarity\")\n",
    "    tab.set_title(2, \"t-SNE Visualization\")\n",
    "    tab.set_title(3, \"Feature Importance\")\n",
    "    \n",
    "    display(tab)\n",
    "\n",
    "# Global dict to store preprocessing options\n",
    "preprocess_options = {\n",
    "    'lowercase': True,\n",
    "    'remove_punct': True,\n",
    "    'remove_stopwords': False,\n",
    "    'lemmatize': False\n",
    "}\n",
    "\n",
    "def interactive_encoding_workshop():\n",
    "    \"\"\"Main function for the interactive encoding workshop\"\"\"\n",
    "    # Text input\n",
    "    text_area = widgets.Textarea(\n",
    "        value='\\n'.join(sample_corpus),\n",
    "        placeholder='Enter text documents, one per line',\n",
    "        description='Documents:',\n",
    "        layout=widgets.Layout(width='100%', height='150px')\n",
    "    )\n",
    "    \n",
    "    # Pre-processing options\n",
    "    lowercase = widgets.Checkbox(value=True, description='Lowercase')\n",
    "    remove_punct = widgets.Checkbox(value=True, description='Remove punctuation')\n",
    "    remove_stopwords = widgets.Checkbox(value=False, description='Remove stopwords')\n",
    "    lemmatize = widgets.Checkbox(value=False, description='Lemmatize')\n",
    "    \n",
    "    preprocess_box = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Text Preprocessing Options:</b>\"),\n",
    "        widgets.HBox([lowercase, remove_punct, remove_stopwords, lemmatize])\n",
    "    ])\n",
    "    \n",
    "    # Encoding method selection\n",
    "    method = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('Bag of Words (Count)', 'count'),\n",
    "            ('TF-IDF', 'tfidf'),\n",
    "            ('One-Hot Encoding', 'onehot'),\n",
    "            ('Hashing Vectorizer', 'hashing')\n",
    "        ],\n",
    "        value='count',\n",
    "        description='Method:'\n",
    "    )\n",
    "    \n",
    "    # Advanced options for encoding methods\n",
    "    max_features = widgets.IntSlider(\n",
    "        value=100,\n",
    "        min=10,\n",
    "        max=1000,\n",
    "        step=10,\n",
    "        description='Max Features:',\n",
    "        disabled=False,\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    binary = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Binary (0/1)',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    ngram_min = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=3,\n",
    "        step=1,\n",
    "        description='N-gram Min:',\n",
    "        disabled=False,\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    ngram_max = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=3,\n",
    "        step=1,\n",
    "        description='N-gram Max:',\n",
    "        disabled=False,\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    use_idf = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Use IDF (for TF-IDF)',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    advanced_options = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Advanced Encoding Options:</b>\"),\n",
    "        widgets.HBox([max_features, binary]),\n",
    "        widgets.HBox([ngram_min, ngram_max, use_idf])\n",
    "    ])\n",
    "    \n",
    "    # Output area for results\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Button to run the encoding\n",
    "    run_button = widgets.Button(\n",
    "        description='Run Encoding',\n",
    "        button_style='success',\n",
    "        tooltip='Click to run the encoding with selected options'\n",
    "    )\n",
    "    \n",
    "    # Function to update preprocessing options\n",
    "    def update_preprocess_options(*args):\n",
    "        preprocess_options['lowercase'] = lowercase.value\n",
    "        preprocess_options['remove_punct'] = remove_punct.value\n",
    "        preprocess_options['remove_stopwords'] = remove_stopwords.value\n",
    "        preprocess_options['lemmatize'] = lemmatize.value\n",
    "    \n",
    "    # Connect preprocessing widgets to the update function\n",
    "    lowercase.observe(update_preprocess_options, names='value')\n",
    "    remove_punct.observe(update_preprocess_options, names='value')\n",
    "    remove_stopwords.observe(update_preprocess_options, names='value')\n",
    "    lemmatize.observe(update_preprocess_options, names='value')\n",
    "    \n",
    "    # Update ngram_max min value based on ngram_min\n",
    "    def update_ngram_max(*args):\n",
    "        ngram_max.min = ngram_min.value\n",
    "        if ngram_max.value < ngram_min.value:\n",
    "            ngram_max.value = ngram_min.value\n",
    "    \n",
    "    ngram_min.observe(update_ngram_max, names='value')\n",
    "    \n",
    "    # Function to handle method changes\n",
    "    def on_method_change(change):\n",
    "        # If one-hot encoding is selected, force binary to True and disable\n",
    "        if change.new == 'onehot':\n",
    "            binary.value = True\n",
    "            binary.disabled = True\n",
    "        else:\n",
    "            binary.disabled = False\n",
    "            # Only for count vectorizer we default to False\n",
    "            if change.new == 'count':\n",
    "                binary.value = False\n",
    "                \n",
    "        # Use IDF only relevant for TF-IDF\n",
    "        use_idf.disabled = change.new != 'tfidf'\n",
    "    \n",
    "    method.observe(on_method_change, names='value')\n",
    "    \n",
    "    # Button click event handler\n",
    "    def on_run_button_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Get input texts\n",
    "            texts = text_area.value.strip().split('\\n')\n",
    "            if not texts or all(not t.strip() for t in texts):\n",
    "                display(HTML(\"<div style='color:red'>Please enter at least one document.</div>\"))\n",
    "                return\n",
    "            \n",
    "            # Update preprocessing options\n",
    "            update_preprocess_options()\n",
    "            \n",
    "            # Display a progress message\n",
    "            display(HTML(\"<div>Processing... Please wait.</div>\"))\n",
    "            \n",
    "            try:\n",
    "                # Time the encoding process\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Perform encoding\n",
    "                X, feature_names, vectorizer = encode_text(\n",
    "                    texts, \n",
    "                    method=method.value,\n",
    "                    max_features=max_features.value if max_features.value > 0 else None,\n",
    "                    binary=binary.value,\n",
    "                    ngram_range=(ngram_min.value, ngram_max.value),\n",
    "                    use_idf=use_idf.value\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                \n",
    "                # Clear the progress message\n",
    "                clear_output()\n",
    "                \n",
    "                # Display statistics\n",
    "                method_label = dict(method.options).get(method.value, method.value.upper())\n",
    "                display(HTML(f\"<h3>Encoding Results: {method_label}</h3>\"))\n",
    "                display(HTML(f\"<div><b>Number of documents:</b> {len(texts)}</div>\"))\n",
    "                display(HTML(f\"<div><b>Number of features:</b> {len(feature_names)}</div>\"))\n",
    "                display(HTML(f\"<div><b>Matrix shape:</b> {X.shape[0]} documents × {X.shape[1]} features</div>\"))\n",
    "                display(HTML(f\"<div><b>Processing time:</b> {end_time - start_time:.4f} seconds</div>\"))\n",
    "                \n",
    "                # Display encoding method explanation\n",
    "                if method.value == 'count':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>Bag of Words:</b> Counts word occurrences in each document. Simple but loses word order and semantic meaning.</div>\"))\n",
    "                elif method.value == 'tfidf':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>TF-IDF:</b> Weights words based on frequency in document and rarity across corpus. Highlights distinctive terms.</div>\"))\n",
    "                elif method.value == 'onehot':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>One-Hot Encoding:</b> Binary representation (0/1) indicating word presence. Loses frequency information but simple to interpret.</div>\"))\n",
    "                elif method.value == 'hashing':\n",
    "                    display(HTML(\"<div style='margin-top:10px'><b>Hashing Vectorizer:</b> Uses a hash function to map terms to indices. Memory efficient but feature names are not interpretable.</div>\"))\n",
    "                \n",
    "                # Display visualizations\n",
    "                visualize_encoding_results(X, feature_names, texts, method.value)\n",
    "                \n",
    "            except Exception as e:\n",
    "                clear_output()\n",
    "                display(HTML(f\"<div style='color:red'>Error: {str(e)}</div>\"))\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    run_button.on_click(on_run_button_click)\n",
    "    \n",
    "    # Layout\n",
    "    display(text_area)\n",
    "    display(preprocess_box)\n",
    "    display(widgets.HBox([method]))\n",
    "    display(advanced_options)\n",
    "    display(run_button)\n",
    "    display(output)\n",
    "\n",
    "# Run the workshop\n",
    "interactive_encoding_workshop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafe960-5adf-4c92-a9ff-3b1b02828000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
