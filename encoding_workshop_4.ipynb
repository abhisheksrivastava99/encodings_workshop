{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e5a4d4-e843-4d3c-8e75-178659d920ff",
   "metadata": {},
   "source": [
    "## 7. Challenges & Edge Cases in Text Encoding\n",
    "\n",
    "Text encoding methods face several challenges when applied to real-world data. This section demonstrates common issues and provides approaches to address them:\n",
    "\n",
    "1. **Out-of-Vocabulary (OOV) Words**: Words in test data not seen during training\n",
    "2. **Rare Words**: Terms that appear very infrequently and may be statistical noise\n",
    "3. **Multi-language Text**: Documents with mixed languages\n",
    "4. **Domain-Specific Vocabulary**: Technical jargon or specialized terminology\n",
    "5. **Misspellings & Variations**: Handling typos and alternative spellings\n",
    "\n",
    "Each challenge includes an interactive demonstration and practical solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2375a5-ee77-4224-bc36-15d3b067732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2197a8cdde1249bca24b05b67f12cd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Common Challenges in Text Encoding</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91693544ac304024a32eb5dcffa3b0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<div>Select a challenge to explore interactive demonstrations and solutions:</div>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1471ee3ff5754c26aa9a2b4195b57818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Challenge:', options=(('Out-of-Vocabulary Words', 'oov'), ('Rare Words', 'rare'), ('Mult…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5ceee2eebd429f9fcbc83b261b0fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Sample data for demonstrations\n",
    "train_docs = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning algorithms analyze data patterns\",\n",
    "    \"Natural language processing techniques extract meaning from text\",\n",
    "    \"Feature engineering improves model performance significantly\",\n",
    "    \"Deep neural networks achieve state-of-the-art results in NLP tasks\"\n",
    "]\n",
    "\n",
    "def demonstrate_oov_challenge():\n",
    "    \"\"\"Demonstrate the Out-of-Vocabulary challenge\"\"\"\n",
    "    # Create a vectorizer and fit on training data\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_docs)\n",
    "    \n",
    "    # Get vocabulary\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Display the vocabulary\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.barh(range(len(vocab)), [1]*len(vocab), tick_label=vocab)\n",
    "    plt.title(\"Vocabulary from Training Data\")\n",
    "    plt.xlabel(\"Term\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Text input for test document\n",
    "    test_doc_input = widgets.Textarea(\n",
    "        value=\"Revolutionary transformer models dominate language understanding benchmarks\",\n",
    "        placeholder='Enter a test document',\n",
    "        description='Test doc:',\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_analyze_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            test_doc = test_doc_input.value\n",
    "            \n",
    "            # Transform the test document\n",
    "            X_test = vectorizer.transform([test_doc])\n",
    "            \n",
    "            # Get tokens from test document\n",
    "            tokens = re.findall(r'\\b\\w+\\b', test_doc.lower())\n",
    "            \n",
    "            # Identify OOV words\n",
    "            oov_words = [word for word in tokens if word not in vocab]\n",
    "            \n",
    "            # Display results\n",
    "            display(HTML(\"<h4>Analysis of Test Document</h4>\"))\n",
    "            display(HTML(f\"<div><b>Total words:</b> {len(tokens)}</div>\"))\n",
    "            display(HTML(f\"<div><b>OOV words:</b> {len(oov_words)} ({len(oov_words)/len(tokens)*100:.1f}%)</div>\"))\n",
    "            \n",
    "            if oov_words:\n",
    "                display(HTML(f\"<div><b>OOV terms:</b> {', '.join(oov_words)}</div>\"))\n",
    "            \n",
    "            # Visualize the encoding\n",
    "            df = pd.DataFrame(X_test.toarray(), columns=vocab)\n",
    "            \n",
    "            plt.figure(figsize=(12, 4))\n",
    "            sns.heatmap(df, cmap=\"YlGnBu\", annot=True, fmt=\"d\")\n",
    "            plt.title(\"Encoding of Test Document\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Suggest solutions\n",
    "            display(HTML(\"<h4>Potential Solutions for OOV Words</h4>\"))\n",
    "            display(HTML(\"\"\"\n",
    "            <ol>\n",
    "                <li><b>Subword Tokenization:</b> Break words into subunits (e.g., WordPiece, BPE)</li>\n",
    "                <li><b>Character-level Encoding:</b> Encode at character level rather than word level</li>\n",
    "                <li><b>Pre-trained Embeddings:</b> Use embeddings with larger vocabulary (e.g., GloVe, Word2Vec)</li>\n",
    "                <li><b>Handle Unknown Token:</b> Add special &lt;UNK&gt; token to represent OOV words</li>\n",
    "                <li><b>Update Vocabulary:</b> Periodically update vocabulary with new terms</li>\n",
    "            </ol>\n",
    "            \"\"\"))\n",
    "    \n",
    "    analyze_button = widgets.Button(\n",
    "        description='Analyze Document',\n",
    "        button_style='success',\n",
    "        tooltip='Analyze the test document for OOV words'\n",
    "    )\n",
    "    \n",
    "    analyze_button.on_click(on_analyze_click)\n",
    "    \n",
    "    display(test_doc_input)\n",
    "    display(analyze_button)\n",
    "    display(output)\n",
    "\n",
    "def demonstrate_rare_words_challenge():\n",
    "    \"\"\"Demonstrate the challenge of rare words\"\"\"\n",
    "    # Define corpus with rare words\n",
    "    corpus_with_rare = [\n",
    "        \"The algorithm performs well on common classification tasks\",\n",
    "        \"Feature extraction is a crucial preprocessing step in NLP\",\n",
    "        \"This is an example of a document with the rare word antidisestablishmentarianism\",\n",
    "        \"Deep learning models require substantial computational resources\",\n",
    "        \"Neural networks have revolutionized image recognition tasks\",\n",
    "        \"This document contains another unusual word like pneumonoultramicroscopicsilicovolcanoconiosis\",\n",
    "        \"Tokenization splits text into individual words or tokens\"\n",
    "    ]\n",
    "    \n",
    "    min_df_slider = widgets.IntSlider(\n",
    "        value=1,\n",
    "        min=1,\n",
    "        max=3,\n",
    "        step=1,\n",
    "        description='min_df:',\n",
    "        tooltip='Minimum document frequency threshold'\n",
    "    )\n",
    "    \n",
    "    max_features_slider = widgets.IntSlider(\n",
    "        value=50,\n",
    "        min=10,\n",
    "        max=100,\n",
    "        step=5,\n",
    "        description='max_features:',\n",
    "        tooltip='Maximum number of features to keep'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_analyze_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Create vectorizers with different settings\n",
    "            vec_all = CountVectorizer()\n",
    "            vec_min_df = CountVectorizer(min_df=min_df_slider.value)\n",
    "            vec_max_features = CountVectorizer(max_features=max_features_slider.value)\n",
    "            \n",
    "            # Fit and transform\n",
    "            X_all = vec_all.fit_transform(corpus_with_rare)\n",
    "            X_min_df = vec_min_df.fit_transform(corpus_with_rare)\n",
    "            X_max_features = vec_max_features.fit_transform(corpus_with_rare)\n",
    "            \n",
    "            # Get vocabularies\n",
    "            vocab_all = vec_all.get_feature_names_out()\n",
    "            vocab_min_df = vec_min_df.get_feature_names_out()\n",
    "            vocab_max_features = vec_max_features.get_feature_names_out()\n",
    "            \n",
    "            # Display vocabulary sizes\n",
    "            display(HTML(\"<h4>Vocabulary Size Comparison</h4>\"))\n",
    "            \n",
    "            sizes = [len(vocab_all), len(vocab_min_df), len(vocab_max_features)]\n",
    "            labels = ['All Terms', f'min_df={min_df_slider.value}', f'max_features={max_features_slider.value}']\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.bar(labels, sizes)\n",
    "            for i, v in enumerate(sizes):\n",
    "                plt.text(i, v + 1, str(v), ha='center')\n",
    "            plt.title(\"Vocabulary Size by Filter Method\")\n",
    "            plt.ylabel(\"Number of Terms\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Show removed rare words\n",
    "            removed_min_df = set(vocab_all) - set(vocab_min_df)\n",
    "            removed_max_features = set(vocab_all) - set(vocab_max_features)\n",
    "            \n",
    "            if removed_min_df:\n",
    "                display(HTML(f\"<div><b>Words removed by min_df={min_df_slider.value}:</b> {', '.join(removed_min_df)}</div>\"))\n",
    "            \n",
    "            if len(removed_max_features) > 10:\n",
    "                display(HTML(f\"<div><b>Words removed by max_features={max_features_slider.value} (showing 10 of {len(removed_max_features)}):</b> {', '.join(list(removed_max_features)[:10])}...</div>\"))\n",
    "            elif removed_max_features:\n",
    "                display(HTML(f\"<div><b>Words removed by max_features={max_features_slider.value}:</b> {', '.join(removed_max_features)}</div>\"))\n",
    "            \n",
    "            # Impact analysis on document similarity\n",
    "            if X_all.shape[0] > 1:\n",
    "                display(HTML(\"<h4>Impact on Document Similarity</h4>\"))\n",
    "                \n",
    "                sim_all = cosine_similarity(X_all)\n",
    "                sim_min_df = cosine_similarity(X_min_df)\n",
    "                sim_max_features = cosine_similarity(X_max_features)\n",
    "                \n",
    "                # Calculate average change in similarity\n",
    "                diff_min_df = np.abs(sim_all - sim_min_df).mean()\n",
    "                diff_max_features = np.abs(sim_all - sim_max_features).mean()\n",
    "                \n",
    "                display(HTML(f\"<div><b>Average change in similarity (min_df):</b> {diff_min_df:.4f}</div>\"))\n",
    "                display(HTML(f\"<div><b>Average change in similarity (max_features):</b> {diff_max_features:.4f}</div>\"))\n",
    "                \n",
    "                # Recommendations\n",
    "                display(HTML(\"<h4>Recommendations for Handling Rare Words</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li><b>Document Frequency Filtering (min_df):</b> Remove terms that appear in fewer than N documents</li>\n",
    "                    <li><b>Vocabulary Pruning (max_features):</b> Keep only the top K most frequent terms</li>\n",
    "                    <li><b>TF-IDF Weighting:</b> Downweight rare terms while still preserving them</li>\n",
    "                    <li><b>Stemming and Lemmatization:</b> Reduce rare variations to common root forms</li>\n",
    "                    <li><b>Word Classes:</b> Map rare technical terms to domain categories</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "    \n",
    "    analyze_button = widgets.Button(\n",
    "        description='Analyze Impact',\n",
    "        button_style='success',\n",
    "        tooltip='Analyze the impact of rare word handling techniques'\n",
    "    )\n",
    "    \n",
    "    analyze_button.on_click(on_analyze_click)\n",
    "    \n",
    "    display(widgets.HTML(\"<h4>Filtering Parameters</h4>\"))\n",
    "    display(widgets.HBox([min_df_slider, max_features_slider]))\n",
    "    display(analyze_button)\n",
    "    display(output)\n",
    "\n",
    "def demonstrate_multilingual_challenge():\n",
    "    \"\"\"Demonstrate challenges with multilingual text\"\"\"\n",
    "    multilingual_texts = [\n",
    "        \"Natural language processing is a subfield of artificial intelligence\",\n",
    "        \"Le traitement du langage naturel est un domaine de l'intelligence artificielle\",\n",
    "        \"El procesamiento del lenguaje natural es un campo de la inteligencia artificial\",\n",
    "        \"自然言語処理は人工知能の一分野です\",\n",
    "        \"Machine learning algorithms can perform classification tasks\",\n",
    "        \"Les algorithmes d'apprentissage automatique peuvent effectuer des tâches de classification\"\n",
    "    ]\n",
    "    \n",
    "    languages = ['English', 'French', 'Spanish', 'Japanese', 'English', 'French']\n",
    "    \n",
    "    approach_selector = widgets.RadioButtons(\n",
    "        options=['Standard CountVectorizer', 'Character n-grams', 'Language-specific Preprocessing'],\n",
    "        description='Approach:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_analyze_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            approach = approach_selector.value\n",
    "            \n",
    "            if approach == 'Standard CountVectorizer':\n",
    "                # Standard word-level approach\n",
    "                vectorizer = CountVectorizer()\n",
    "                X = vectorizer.fit_transform(multilingual_texts)\n",
    "                vocab = vectorizer.get_feature_names_out()\n",
    "                \n",
    "                # Display vocabulary\n",
    "                display(HTML(f\"<div><b>Vocabulary size:</b> {len(vocab)}</div>\"))\n",
    "                display(HTML(f\"<div><b>Sample terms:</b> {', '.join(vocab[:20])}...</div>\"))\n",
    "                \n",
    "                # Analyze cross-language similarity\n",
    "                sim_matrix = cosine_similarity(X)\n",
    "                \n",
    "                # Plot similarity matrix\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", \n",
    "                            xticklabels=[f\"{lang} ({i+1})\" for i, lang in enumerate(languages)],\n",
    "                            yticklabels=[f\"{lang} ({i+1})\" for i, lang in enumerate(languages)])\n",
    "                plt.title(\"Cross-language Document Similarity (Word-level)\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Analysis\n",
    "                display(HTML(\"<h4>Observations</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li>Documents in the same language have higher similarity</li>\n",
    "                    <li>Cross-language similarity is very low, even for semantically identical content</li>\n",
    "                    <li>Standard word-level tokenization treats each language's vocabulary as completely separate</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "            elif approach == 'Character n-grams':\n",
    "                # Character n-gram approach\n",
    "                char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "                X_char = char_vectorizer.fit_transform(multilingual_texts)\n",
    "                char_vocab = char_vectorizer.get_feature_names_out()\n",
    "                \n",
    "                # Display vocabulary\n",
    "                display(HTML(f\"<div><b>Character n-gram vocabulary size:</b> {len(char_vocab)}</div>\"))\n",
    "                display(HTML(f\"<div><b>Sample n-grams:</b> {', '.join(char_vocab[:20])}...</div>\"))\n",
    "                \n",
    "                # Analyze cross-language similarity\n",
    "                char_sim_matrix = cosine_similarity(X_char)\n",
    "                \n",
    "                # Plot similarity matrix\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(char_sim_matrix, annot=True, fmt=\".2f\", \n",
    "                            xticklabels=[f\"{lang} ({i+1})\" for i, lang in enumerate(languages)],\n",
    "                            yticklabels=[f\"{lang} ({i+1})\" for i, lang in enumerate(languages)])\n",
    "                plt.title(\"Cross-language Document Similarity (Character n-grams)\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Analysis\n",
    "                display(HTML(\"<h4>Observations</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li>Character n-grams capture some cross-language similarities, especially for related languages</li>\n",
    "                    <li>Languages with shared character sets (Latin alphabet) show higher cross-language similarity</li>\n",
    "                    <li>Character n-grams are more robust for multilingual text but less semantically meaningful</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "            elif approach == 'Language-specific Preprocessing':\n",
    "                # Language detection and tokenization simulation\n",
    "                display(HTML(\"<h4>Language-specific Preprocessing</h4>\"))\n",
    "                display(HTML(\"<div>In a real application, we would:</div>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ol>\n",
    "                    <li>Detect the language of each document (e.g., using langdetect)</li>\n",
    "                    <li>Apply language-specific preprocessing (stemmers, stopwords)</li>\n",
    "                    <li>Either:\n",
    "                        <ul>\n",
    "                            <li>Process each language separately, or</li>\n",
    "                            <li>Use cross-lingual embeddings to unify representations</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ol>\n",
    "                \"\"\"))\n",
    "                \n",
    "                # Display detected languages\n",
    "                for i, (text, lang) in enumerate(zip(multilingual_texts, languages)):\n",
    "                    display(HTML(f\"<div><b>Document {i+1}:</b> Detected as {lang}</div>\"))\n",
    "                    display(HTML(f\"<div><i>{text[:50]}...</i></div>\"))\n",
    "                \n",
    "                # Recommendations\n",
    "                display(HTML(\"<h4>Best Practices for Multilingual Text</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li><b>Language Detection:</b> Use language identification before processing</li>\n",
    "                    <li><b>Language-specific Resources:</b> Apply appropriate stemmers and stopword lists</li>\n",
    "                    <li><b>Cross-lingual Embeddings:</b> Use models like mBERT, XLM-R that work across languages</li>\n",
    "                    <li><b>Character n-grams:</b> Use for language-agnostic processing of related languages</li>\n",
    "                    <li><b>Translation:</b> Convert all text to a common language before encoding</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "    \n",
    "    analyze_button = widgets.Button(\n",
    "        description='Analyze Approach',\n",
    "        button_style='success',\n",
    "        tooltip='Analyze the selected approach for multilingual text'\n",
    "    )\n",
    "    \n",
    "    analyze_button.on_click(on_analyze_click)\n",
    "    \n",
    "    display(widgets.HTML(\"<h4>Multilingual Text Encoding Approaches</h4>\"))\n",
    "    display(approach_selector)\n",
    "    display(analyze_button)\n",
    "    display(output)\n",
    "\n",
    "def demonstrate_domain_vocabulary_challenge():\n",
    "    \"\"\"Demonstrate challenges with domain-specific vocabulary\"\"\"\n",
    "    # Domain-specific text examples\n",
    "    general_text = \"The computer processes data through its cpu and stores information in memory.\"\n",
    "    medical_text = \"The patient presented with acute myocardial infarction requiring immediate coronary angiography.\"\n",
    "    legal_text = \"The defendant filed a motion to dismiss pursuant to Rule 12(b)(6) alleging failure to state a claim.\"\n",
    "    tech_text = \"We implemented a RESTful API using Node.js with MongoDB for backend persistence.\"\n",
    "    \n",
    "    domain_texts = [general_text, medical_text, legal_text, tech_text]\n",
    "    domains = ['General', 'Medical', 'Legal', 'Technical']\n",
    "    \n",
    "    method_selector = widgets.RadioButtons(\n",
    "        options=['Standard TF-IDF', 'Domain Adaptation'],\n",
    "        description='Method:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_analyze_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            method = method_selector.value\n",
    "            \n",
    "            if method == 'Standard TF-IDF':\n",
    "                # Standard TF-IDF approach\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                X = vectorizer.fit_transform(domain_texts)\n",
    "                vocab = vectorizer.get_feature_names_out()\n",
    "                \n",
    "                # Display term frequency & importance\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                df = pd.DataFrame(X.toarray(), index=domains, columns=feature_names)\n",
    "                \n",
    "                # Find most important terms per domain\n",
    "                top_terms_per_domain = {}\n",
    "                for i, domain in enumerate(domains):\n",
    "                    # Get non-zero terms and their values\n",
    "                    domain_tfidf = X[i].toarray()[0]\n",
    "                    term_importance = [(term, score) for term, score in zip(feature_names, domain_tfidf) if score > 0]\n",
    "                    # Sort by importance\n",
    "                    sorted_terms = sorted(term_importance, key=lambda x: x[1], reverse=True)\n",
    "                    top_terms_per_domain[domain] = sorted_terms[:5]\n",
    "                \n",
    "                # Plot top terms by domain\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                for i, domain in enumerate(domains):\n",
    "                    plt.subplot(2, 2, i+1)\n",
    "                    terms = [t[0] for t in top_terms_per_domain[domain]]\n",
    "                    scores = [t[1] for t in top_terms_per_domain[domain]]\n",
    "                    plt.barh(terms, scores)\n",
    "                    plt.title(f\"Top Terms: {domain} Domain\")\n",
    "                    plt.xlabel(\"TF-IDF Score\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Analysis\n",
    "                display(HTML(\"<h4>Observations</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li>Domain-specific terms receive high TF-IDF scores</li>\n",
    "                    <li>Standard TF-IDF can identify domain-specific terminology</li>\n",
    "                    <li>Domain terms like \"myocardial\", \"infarction\", \"pursuant\", \"RESTful\" stand out</li>\n",
    "                    <li>Challenge: Rare but important domain terms may be underrepresented in general corpora</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "            elif method == 'Domain Adaptation':\n",
    "                display(HTML(\"<h4>Domain Adaptation Approaches</h4>\"))\n",
    "                display(HTML(\"<div>In real applications, domain adaptation techniques include:</div>\"))\n",
    "                \n",
    "                display(HTML(\"\"\"\n",
    "                <h5>1. Domain-specific Vocabulary Enhancement</h5>\n",
    "                <ul>\n",
    "                    <li><b>Example:</b> Augmenting vocabulary with domain dictionaries</li>\n",
    "                    <li><b>Technique:</b> Add domain-specific terms to ensure they're recognized</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "                # Simulate domain dictionary augmentation\n",
    "                medical_terms = ['myocardial', 'infarction', 'coronary', 'angiography', 'patient']\n",
    "                legal_terms = ['defendant', 'motion', 'dismiss', 'pursuant', 'claim']\n",
    "                tech_terms = ['api', 'restful', 'node.js', 'mongodb', 'backend']\n",
    "                \n",
    "                display(HTML(\"<div><b>Medical Dictionary:</b> \" + \", \".join(medical_terms) + \"</div>\"))\n",
    "                display(HTML(\"<div><b>Legal Dictionary:</b> \" + \", \".join(legal_terms) + \"</div>\"))\n",
    "                display(HTML(\"<div><b>Tech Dictionary:</b> \" + \", \".join(tech_terms) + \"</div>\"))\n",
    "                \n",
    "                display(HTML(\"\"\"\n",
    "                <h5>2. Domain-specific Word Embeddings</h5>\n",
    "                <ul>\n",
    "                    <li><b>Example:</b> Training embeddings on domain-specific corpora</li>\n",
    "                    <li><b>Technique:</b> Use medical papers for medical NLP, legal documents for legal NLP</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "                # Create visualization of domain-specific embeddings\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                \n",
    "                # Simulate 2D embeddings for visualization\n",
    "                np.random.seed(42)\n",
    "                words = medical_terms + legal_terms + tech_terms\n",
    "                # Generate random 2D coordinates\n",
    "                coords = np.random.randn(len(words), 2)\n",
    "                # Add domain clustering effect\n",
    "                for i in range(len(medical_terms)):\n",
    "                    coords[i] += [-3, 2]\n",
    "                for i in range(len(medical_terms), len(medical_terms) + len(legal_terms)):\n",
    "                    coords[i] += [3, 2]\n",
    "                for i in range(len(medical_terms) + len(legal_terms), len(words)):\n",
    "                    coords[i] += [0, -3]\n",
    "                \n",
    "                # Plot simulated embeddings\n",
    "                plt.scatter(coords[:len(medical_terms), 0], coords[:len(medical_terms), 1], c='r', label='Medical')\n",
    "                plt.scatter(coords[len(medical_terms):len(medical_terms)+len(legal_terms), 0], \n",
    "                           coords[len(medical_terms):len(medical_terms)+len(legal_terms), 1], c='b', label='Legal')\n",
    "                plt.scatter(coords[len(medical_terms)+len(legal_terms):, 0], \n",
    "                           coords[len(medical_terms)+len(legal_terms):, 1], c='g', label='Technical')\n",
    "                \n",
    "                # Add word labels\n",
    "                for i, word in enumerate(words):\n",
    "                    plt.annotate(word, (coords[i, 0], coords[i, 1]))\n",
    "                \n",
    "                plt.title(\"Simulated Domain-specific Word Embeddings\")\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Recommendations\n",
    "                display(HTML(\"<h4>Best Practices for Domain-specific Vocabulary</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li><b>Domain-specific Corpora:</b> Train on text from the target domain</li>\n",
    "                    <li><b>Domain Dictionaries:</b> Integrate specialized terminology lists</li>\n",
    "                    <li><b>Transfer Learning:</b> Fine-tune pre-trained models on domain texts</li>\n",
    "                    <li><b>Entity Recognition:</b> Develop domain-specific NER models</li>\n",
    "                    <li><b>Expert Validation:</b> Have domain experts verify term importance</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "    \n",
    "    analyze_button = widgets.Button(\n",
    "        description='Analyze Method',\n",
    "        button_style='success',\n",
    "        tooltip='Analyze the selected method for domain-specific vocabulary'\n",
    "    )\n",
    "    \n",
    "    analyze_button.on_click(on_analyze_click)\n",
    "    \n",
    "    display(widgets.HTML(\"<h4>Domain-specific Vocabulary Encoding</h4>\"))\n",
    "    display(method_selector)\n",
    "    display(analyze_button)\n",
    "    display(output)\n",
    "    display(HTML(\"<div><b>Text Examples:</b></div>\"))\n",
    "    for i, (text, domain) in enumerate(zip(domain_texts, domains)):\n",
    "        display(HTML(f\"<div><b>{domain}:</b> {text}</div>\"))\n",
    "\n",
    "# Continuing from the previous code in Section 7\n",
    "\n",
    "def demonstrate_spelling_variations_challenge():\n",
    "    \"\"\"Demonstrate challenges with spelling variations and misspellings\"\"\"\n",
    "    # Text with spelling variations\n",
    "    variation_texts = [\n",
    "        \"The color of the center flag is blue\",\n",
    "        \"The colour of the centre flag is blue\",\n",
    "        \"The color of the center flag is blu\",\n",
    "        \"The culor of the senter flag is blue\",\n",
    "        \"The color of the center phlag is blue\"\n",
    "    ]\n",
    "    \n",
    "    variation_types = [\n",
    "        \"Standard American English\",\n",
    "        \"British English Spelling\",\n",
    "        \"Misspelling (missing letter)\",\n",
    "        \"Misspelling (wrong letters)\",\n",
    "        \"Phonetic Spelling Variation\"\n",
    "    ]\n",
    "    \n",
    "    approach_selector = widgets.RadioButtons(\n",
    "        options=['Standard Encoding', 'Character n-grams', 'Spelling Normalization'],\n",
    "        description='Approach:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_analyze_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            approach = approach_selector.value\n",
    "            \n",
    "            if approach == 'Standard Encoding':\n",
    "                # Standard word-level approach\n",
    "                vectorizer = CountVectorizer()\n",
    "                X = vectorizer.fit_transform(variation_texts)\n",
    "                vocab = vectorizer.get_feature_names_out()\n",
    "                \n",
    "                # Display vocabulary and encoding\n",
    "                display(HTML(f\"<div><b>Vocabulary size:</b> {len(vocab)}</div>\"))\n",
    "                display(HTML(f\"<div><b>Vocabulary:</b> {', '.join(sorted(vocab))}</div>\"))\n",
    "                \n",
    "                # Show document-term matrix\n",
    "                df = pd.DataFrame(X.toarray(), index=variation_types, columns=vocab)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "                plt.title(\"Document-Term Matrix with Spelling Variations\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Document similarity\n",
    "                sim_matrix = cosine_similarity(X)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", \n",
    "                            xticklabels=[f\"Text {i+1}\" for i in range(len(variation_texts))],\n",
    "                            yticklabels=variation_types)\n",
    "                plt.title(\"Document Similarity with Spelling Variations\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Analysis\n",
    "                display(HTML(\"<h4>Observations</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li>Each spelling variation creates a separate term in the vocabulary</li>\n",
    "                    <li>\"color\"/\"colour\", \"center\"/\"centre\", \"blu\"/\"blue\" are treated as completely different</li>\n",
    "                    <li>Similar sentences have reduced similarity scores due to spelling differences</li>\n",
    "                    <li>Standard encoding is very sensitive to spelling variations</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "            elif approach == 'Character n-grams':\n",
    "                # Character n-gram approach\n",
    "                char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 4))\n",
    "                X_char = char_vectorizer.fit_transform(variation_texts)\n",
    "                \n",
    "                # Document similarity with character n-grams\n",
    "                char_sim_matrix = cosine_similarity(X_char)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(char_sim_matrix, annot=True, fmt=\".2f\", \n",
    "                            xticklabels=[f\"Text {i+1}\" for i in range(len(variation_texts))],\n",
    "                            yticklabels=variation_types)\n",
    "                plt.title(\"Document Similarity with Character n-grams\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Highlight character n-grams for a word with variations\n",
    "                display(HTML(\"<h4>Character n-grams Example</h4>\"))\n",
    "                \n",
    "                variations = [\"center\", \"centre\", \"senter\", \"centr\"]\n",
    "                ngrams = {}\n",
    "                \n",
    "                for word in variations:\n",
    "                    word_ngrams = []\n",
    "                    for i in range(len(word)-2):\n",
    "                        word_ngrams.append(word[i:i+3])\n",
    "                    ngrams[word] = word_ngrams\n",
    "                \n",
    "                for word, grams in ngrams.items():\n",
    "                    display(HTML(f\"<div><b>{word}:</b> {', '.join(grams)}</div>\"))\n",
    "                \n",
    "                # Analysis\n",
    "                display(HTML(\"<h4>Observations</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li>Character n-grams capture partial matches between spelling variations</li>\n",
    "                    <li>Similarity scores are much higher despite spelling differences</li>\n",
    "                    <li>More robust to typos and regional spelling variations</li>\n",
    "                    <li>Trade-off: loses some word-level semantic precision</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "                \n",
    "            elif approach == 'Spelling Normalization':\n",
    "                # Spelling normalization (simulated)\n",
    "                display(HTML(\"<h4>Spelling Normalization Techniques</h4>\"))\n",
    "                \n",
    "                # Define a simple spelling correction function for demonstration\n",
    "                def simple_normalize(text):\n",
    "                    # Simple replacements for demonstration purposes\n",
    "                    replacements = {\n",
    "                        'colour': 'color',\n",
    "                        'centre': 'center',\n",
    "                        'blu': 'blue',\n",
    "                        'culor': 'color',\n",
    "                        'senter': 'center',\n",
    "                        'phlag': 'flag'\n",
    "                    }\n",
    "                    \n",
    "                    words = text.lower().split()\n",
    "                    normalized = []\n",
    "                    corrections = []\n",
    "                    \n",
    "                    for word in words:\n",
    "                        if word in replacements:\n",
    "                            normalized.append(replacements[word])\n",
    "                            corrections.append((word, replacements[word]))\n",
    "                        else:\n",
    "                            normalized.append(word)\n",
    "                    \n",
    "                    return ' '.join(normalized), corrections\n",
    "                \n",
    "                # Apply normalization\n",
    "                normalized_texts = []\n",
    "                all_corrections = []\n",
    "                \n",
    "                for text in variation_texts:\n",
    "                    norm_text, corrections = simple_normalize(text)\n",
    "                    normalized_texts.append(norm_text)\n",
    "                    all_corrections.append(corrections)\n",
    "                \n",
    "                # Display normalized texts and corrections\n",
    "                for i, (orig, norm, corr) in enumerate(zip(variation_texts, normalized_texts, all_corrections)):\n",
    "                    display(HTML(f\"<div><b>Original ({variation_types[i]}):</b> {orig}</div>\"))\n",
    "                    display(HTML(f\"<div><b>Normalized:</b> {norm}</div>\"))\n",
    "                    if corr:\n",
    "                        display(HTML(f\"<div><b>Corrections:</b> {', '.join([f'{old} → {new}' for old, new in corr])}</div>\"))\n",
    "                    display(HTML(\"<hr>\"))\n",
    "                \n",
    "                # Compare similarity before and after normalization\n",
    "                vectorizer = CountVectorizer()\n",
    "                X_orig = vectorizer.fit_transform(variation_texts)\n",
    "                sim_orig = cosine_similarity(X_orig)\n",
    "                \n",
    "                vectorizer_norm = CountVectorizer()\n",
    "                X_norm = vectorizer_norm.fit_transform(normalized_texts)\n",
    "                sim_norm = cosine_similarity(X_norm)\n",
    "                \n",
    "                # Plot comparison\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "                \n",
    "                sns.heatmap(sim_orig, annot=True, fmt=\".2f\", \n",
    "                            xticklabels=[f\"Text {i+1}\" for i in range(len(variation_texts))],\n",
    "                            yticklabels=[f\"Text {i+1}\" for i in range(len(variation_texts))],\n",
    "                            ax=ax1)\n",
    "                ax1.set_title(\"Before Normalization\")\n",
    "                \n",
    "                sns.heatmap(sim_norm, annot=True, fmt=\".2f\", \n",
    "                            xticklabels=[f\"Text {i+1}\" for i in range(len(normalized_texts))],\n",
    "                            yticklabels=[f\"Text {i+1}\" for i in range(len(normalized_texts))],\n",
    "                            ax=ax2)\n",
    "                ax2.set_title(\"After Normalization\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Analysis\n",
    "                display(HTML(\"<h4>Spelling Normalization Techniques</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ol>\n",
    "                    <li><b>Dictionary-based Correction:</b> Map misspellings to correct forms using dictionaries</li>\n",
    "                    <li><b>Edit Distance Methods:</b> Levenshtein distance to find closest dictionary words</li>\n",
    "                    <li><b>Phonetic Algorithms:</b> Soundex, Metaphone to handle phonetic variations</li>\n",
    "                    <li><b>Statistical Methods:</b> Use language models to predict correct forms</li>\n",
    "                    <li><b>Context-aware Correction:</b> Use surrounding words to disambiguate</li>\n",
    "                </ol>\n",
    "                \"\"\"))\n",
    "                \n",
    "                # Recommendations\n",
    "                display(HTML(\"<h4>Best Practices for Handling Spelling Variations</h4>\"))\n",
    "                display(HTML(\"\"\"\n",
    "                <ul>\n",
    "                    <li><b>Preprocessing:</b> Apply spelling normalization before encoding</li>\n",
    "                    <li><b>Character n-grams:</b> Use for robustness when spelling correction isn't feasible</li>\n",
    "                    <li><b>Phonetic Encoding:</b> Consider for names and terms with many variations</li>\n",
    "                    <li><b>Hybrid Approaches:</b> Combine word and character-level representations</li>\n",
    "                    <li><b>Regional Variations:</b> Standardize spelling conventions (e.g., US vs. UK English)</li>\n",
    "                </ul>\n",
    "                \"\"\"))\n",
    "    \n",
    "    analyze_button = widgets.Button(\n",
    "        description='Analyze Approach',\n",
    "        button_style='success',\n",
    "        tooltip='Analyze the selected approach for spelling variations'\n",
    "    )\n",
    "    \n",
    "    analyze_button.on_click(on_analyze_click)\n",
    "    \n",
    "    display(widgets.HTML(\"<h4>Handling Spelling Variations and Misspellings</h4>\"))\n",
    "    display(approach_selector)\n",
    "    display(analyze_button)\n",
    "    display(output)\n",
    "\n",
    "# Function to select and demonstrate a specific challenge\n",
    "def demonstrate_encoding_challenges():\n",
    "    challenge_selector = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('Out-of-Vocabulary Words', 'oov'),\n",
    "            ('Rare Words', 'rare'),\n",
    "            ('Multilingual Text', 'multilingual'),\n",
    "            ('Domain-Specific Vocabulary', 'domain'),\n",
    "            ('Spelling Variations & Misspellings', 'spelling')\n",
    "        ],\n",
    "        value='oov',\n",
    "        description='Challenge:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    challenge_output = widgets.Output()\n",
    "    \n",
    "    def on_challenge_select(change):\n",
    "        with challenge_output:\n",
    "            clear_output()\n",
    "            \n",
    "            if change.new == 'oov':\n",
    "                demonstrate_oov_challenge()\n",
    "            elif change.new == 'rare':\n",
    "                demonstrate_rare_words_challenge()\n",
    "            elif change.new == 'multilingual':\n",
    "                demonstrate_multilingual_challenge()\n",
    "            elif change.new == 'domain':\n",
    "                demonstrate_domain_vocabulary_challenge()\n",
    "            elif change.new == 'spelling':\n",
    "                demonstrate_spelling_variations_challenge()\n",
    "    \n",
    "    challenge_selector.observe(on_challenge_select, names='value')\n",
    "    \n",
    "    display(widgets.HTML(\"<h3>Common Challenges in Text Encoding</h3>\"))\n",
    "    display(widgets.HTML(\"<div>Select a challenge to explore interactive demonstrations and solutions:</div>\"))\n",
    "    display(challenge_selector)\n",
    "    display(challenge_output)\n",
    "    \n",
    "    # Trigger the first challenge to display\n",
    "    with challenge_output:\n",
    "        demonstrate_oov_challenge()\n",
    "\n",
    "# Run the challenge demonstrations\n",
    "demonstrate_encoding_challenges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de42b9-55b2-4bd7-a372-ef44b3150143",
   "metadata": {},
   "source": [
    "## 8. Conclusion & Further Reading\n",
    "\n",
    "### Summary of Text Encoding Approaches\n",
    "\n",
    "This workshop has explored various text encoding methods, from basic techniques to advanced implementations, with a focus on interactive exploration and practical understanding:\n",
    "\n",
    "1. **Bag of Words (Count Vectorization)**\n",
    "   - Simple frequency-based representation\n",
    "   - Preserves term importance through counts\n",
    "   - Loses word order and context\n",
    "\n",
    "2. **TF-IDF Encoding**\n",
    "   - Balances term frequency with corpus-wide importance\n",
    "   - Highlights distinctive terms in documents\n",
    "   - Better for information retrieval and document similarity\n",
    "\n",
    "3. **One-Hot Encoding**\n",
    "   - Binary representation of term presence\n",
    "   - Simple but highly sparse\n",
    "   - Equal weight to all terms regardless of frequency\n",
    "\n",
    "4. **Hashing Vectorizer**\n",
    "   - Memory-efficient approach for large vocabularies\n",
    "   - No vocabulary mapping to maintain\n",
    "   - Potential for hash collisions\n",
    "\n",
    "5. **Character n-grams**\n",
    "   - Robust to spelling variations and morphological differences\n",
    "   - Works across related languages\n",
    "   - Captures subword patterns\n",
    "\n",
    "6. **Word Embeddings** (covered conceptually)\n",
    "   - Dense vector representations that capture semantic relationships\n",
    "   - Significantly lower dimensionality than sparse representations\n",
    "   - Captures word similarities and analogies\n",
    "\n",
    "### Key Encoding Considerations\n",
    "\n",
    "Throughout this workshop, we've highlighted several important considerations when choosing and implementing text encoding methods:\n",
    "\n",
    "- **Dimensionality**: Balance between feature richness and computational efficiency\n",
    "- **Sparsity**: Impact on storage requirements and model performance\n",
    "- **Interpretability**: Ability to understand and explain the encoded features\n",
    "- **Domain-specificity**: Adaptation to specialized vocabulary and linguistic patterns\n",
    "- **Preprocessing dependencies**: Effect of tokenization, stemming, etc. on encoding results\n",
    "- **Language dependencies**: Handling multiple languages and cross-lingual applications\n",
    "- **Scalability**: Performance with increasing vocabulary size and document collections\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "To deepen your understanding of text encoding methods, we recommend exploring these resources:\n",
    "\n",
    "#### Books\n",
    "- \"Speech and Language Processing\" by Daniel Jurafsky & James H. Martin\n",
    "- \"Natural Language Processing with Python\" by Steven Bird, Ewan Klein & Edward Loper\n",
    "- \"Introduction to Information Retrieval\" by Christopher D. Manning, Prabhakar Raghavan & Hinrich Schütze\n",
    "\n",
    "#### Research Papers\n",
    "- Mikolov et al. (2013): \"Distributed Representations of Words and Phrases and their Compositionality\"\n",
    "- Pennington et al. (2014): \"GloVe: Global Vectors for Word Representation\"\n",
    "- Bojanowski et al. (2017): \"Enriching Word Vectors with Subword Information\"\n",
    "- Devlin et al. (2019): \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
    "\n",
    "#### Online Resources\n",
    "- [scikit-learn Documentation: Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "- [spaCy Documentation: Processing Text](https://spacy.io/usage/processing-text)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [Stanford NLP Group Resources](https://nlp.stanford.edu/resources.html)\n",
    "\n",
    "### Next Steps in Your NLP Journey\n",
    "\n",
    "This workshop provides a foundation for text encoding. To continue building your NLP skills:\n",
    "\n",
    "1. **Experiment with Pre-trained Models**: Explore BERT, GPT, and other transformer-based models\n",
    "2. **Build End-to-End Applications**: Apply these encoding techniques to real-world NLP tasks\n",
    "3. **Explore Multimodal Representations**: Combine text with other data types like images or audio\n",
    "4. **Contribute to Open Source**: Many NLP libraries welcome contributions and improvements\n",
    "5. **Stay Current with Research**: The field evolves rapidly; follow conferences like ACL, EMNLP, and NeurIPS\n",
    "\n",
    "Thank you for participating in this interactive workshop on text encodings in NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c98dbf7-483a-4c0e-b8aa-de77b69c0707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0d6d9d692b4b5ea6b96bb5d8e5c9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Test Your Knowledge: Text Encoding Quiz</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888dfd271f9e4ce29c6fae0093227bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<div>Complete this short quiz to reinforce key concepts from the workshop:</div>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfc085810094d3ebbee3b6173e1249b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d42028616c43e495ac80c5cce2ce7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=(), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b302a9b47464b9e414a92b3742364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Check Answer', style=ButtonStyle()), Button(button_sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81df07d7d9af48ce8b841e7a28d77455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebccc6151de4104ab0da216aaa575e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_encoding_quiz():\n",
    "    \"\"\"Interactive quiz to test knowledge of text encoding concepts\"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        {\n",
    "            \"question\": \"Which encoding method gives higher weight to rare terms that appear in few documents?\",\n",
    "            \"options\": [\"Bag of Words\", \"TF-IDF\", \"One-Hot Encoding\", \"Character n-grams\"],\n",
    "            \"answer\": 1,  # TF-IDF\n",
    "            \"explanation\": \"TF-IDF specifically downweights common terms that appear in many documents while giving higher weight to distinctive terms that appear in few documents.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the main advantage of using hashing vectorization?\",\n",
    "            \"options\": [\"Higher accuracy\", \"Memory efficiency with large vocabularies\", \"Better semantic understanding\", \"Slower processing speed\"],\n",
    "            \"answer\": 1,  # Memory efficiency\n",
    "            \"explanation\": \"Hashing vectorizer is memory-efficient because it doesn't need to store a vocabulary mapping. It uses a hash function to map terms directly to feature indices.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Which encoding method is most robust to spelling variations?\",\n",
    "            \"options\": [\"Bag of Words\", \"TF-IDF\", \"One-Hot Encoding\", \"Character n-grams\"],\n",
    "            \"answer\": 3,  # Character n-grams\n",
    "            \"explanation\": \"Character n-grams capture subword patterns and are more robust to spelling variations because misspelled words often share many of the same character sequences.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is a common challenge with the Bag of Words approach?\",\n",
    "            \"options\": [\"Too slow to compute\", \"Loses word order information\", \"Requires too much memory\", \"Only works with English text\"],\n",
    "            \"answer\": 1,  # Loses word order\n",
    "            \"explanation\": \"Bag of Words treats text as an unordered collection of words, discarding all information about word order and context, which can be critical for understanding meaning.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Which preprocessing step would most help with handling Out-of-Vocabulary (OOV) words?\",\n",
    "            \"options\": [\"Converting to lowercase\", \"Stemming/Lemmatization\", \"Using subword tokenization\", \"Removing stopwords\"],\n",
    "            \"answer\": 2,  # Subword tokenization\n",
    "            \"explanation\": \"Subword tokenization (like BPE or WordPiece) breaks words into smaller units, allowing the model to handle previously unseen words by combining familiar subword pieces.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Randomize question order\n",
    "    random.shuffle(questions)\n",
    "    \n",
    "    # Create widgets\n",
    "    question_text = widgets.HTML()\n",
    "    options = widgets.RadioButtons(options=[])\n",
    "    check_button = widgets.Button(description=\"Check Answer\", button_style=\"info\")\n",
    "    next_button = widgets.Button(description=\"Next Question\", button_style=\"success\")\n",
    "    result_text = widgets.HTML()\n",
    "    progress_text = widgets.HTML()\n",
    "    \n",
    "    current_question = 0\n",
    "    score = 0\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def show_question(idx):\n",
    "        nonlocal current_question\n",
    "        current_question = idx\n",
    "        \n",
    "        q = questions[idx]\n",
    "        question_text.value = f\"<b>Question {idx+1}/{len(questions)}:</b> {q['question']}\"\n",
    "        options.options = q['options']\n",
    "        options.index = None  # Reset selection\n",
    "        result_text.value = \"\"\n",
    "        progress_text.value = f\"<div>Score: {score}/{len(questions)}</div>\"\n",
    "        \n",
    "        check_button.disabled = False\n",
    "        next_button.disabled = True\n",
    "    \n",
    "    def check_answer(b):\n",
    "        q = questions[current_question]\n",
    "        if options.index is None:\n",
    "            result_text.value = \"<div style='color:red'>Please select an answer!</div>\"\n",
    "            return\n",
    "            \n",
    "        if options.index == q['answer']:\n",
    "            nonlocal score\n",
    "            score += 1\n",
    "            result_text.value = f\"<div style='color:green'><b>Correct!</b> {q['explanation']}</div>\"\n",
    "        else:\n",
    "            result_text.value = f\"<div style='color:red'><b>Incorrect.</b> The correct answer is '{q['options'][q['answer']]}'. {q['explanation']}</div>\"\n",
    "        \n",
    "        progress_text.value = f\"<div>Score: {score}/{len(questions)}</div>\"\n",
    "        check_button.disabled = True\n",
    "        next_button.disabled = False\n",
    "    \n",
    "    def next_question(b):\n",
    "        if current_question < len(questions) - 1:\n",
    "            show_question(current_question + 1)\n",
    "        else:\n",
    "            # Quiz completed\n",
    "            question_text.value = \"<h3>Quiz Completed!</h3>\"\n",
    "            options.options = []\n",
    "            result_text.value = f\"<div><b>Final Score:</b> {score}/{len(questions)}</div>\"\n",
    "            \n",
    "            if score == len(questions):\n",
    "                result_text.value += \"<div style='color:green'>Perfect score! You've mastered text encoding concepts!</div>\"\n",
    "            elif score >= len(questions) * 0.8:\n",
    "                result_text.value += \"<div style='color:green'>Great job! You have a strong understanding of text encoding.</div>\"\n",
    "            elif score >= len(questions) * 0.6:\n",
    "                result_text.value += \"<div style='color:blue'>Good effort! Review the areas you missed to strengthen your knowledge.</div>\"\n",
    "            else:\n",
    "                result_text.value += \"<div style='color:orange'>Consider reviewing the workshop material to strengthen your understanding of text encoding concepts.</div>\"\n",
    "                \n",
    "            check_button.disabled = True\n",
    "            next_button.disabled = True\n",
    "    \n",
    "    check_button.on_click(check_answer)\n",
    "    next_button.on_click(next_question)\n",
    "    \n",
    "    # Create layout\n",
    "    display(widgets.HTML(\"<h3>Test Your Knowledge: Text Encoding Quiz</h3>\"))\n",
    "    display(widgets.HTML(\"<div>Complete this short quiz to reinforce key concepts from the workshop:</div>\"))\n",
    "    display(question_text)\n",
    "    display(options)\n",
    "    display(widgets.HBox([check_button, next_button]))\n",
    "    display(result_text)\n",
    "    display(progress_text)\n",
    "    \n",
    "    # Show first question\n",
    "    show_question(0)\n",
    "\n",
    "# Run the quiz\n",
    "text_encoding_quiz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ccc81-9c94-4635-8a01-68163767bf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
